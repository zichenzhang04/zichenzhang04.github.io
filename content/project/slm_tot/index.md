---
title: Babysitting a Small Language Model through One-Step Tree-of-Thoughts Knowledge Distillation

# subtitle: A Subtitle

summary: <div><p style=color:gray>Anurag Renduchintala*, Adi Mahesh*, <b>Zichen Zhang</b>*, Zimo Si*, Shangjun Meng*, Samuel Fang*.<br></p></div>We introduce the One-Step Tree-of-Thoughts framework, a simplified prompting method that distills multi-step reasoning into a single structured prompt, and demonstrates how knowledge distillation can transfer this reasoning capability from Large Language Models to Small Language Models with much less parameters, enabling significant improvements reasoning performance, beating GPT-4o and GPT-4, as shown on the model's performance on Game of 24.

tags:
  - NLP
  - LLM
  - ML

date: "2024-11-28"

links:
  - icon_pack: fab
    name: Poster
    url: uploads/slm_tot_poster.pdf
  - icon_pack: fab
    name: Code
    url: "https://github.com/zichenzhang04/slm-tot"
  - icon_pack: fab
    name: Slides
    url: "https://docs.google.com/presentation/d/1Z6alEtiIT-FiFVZRdf5-Wq34b7H2LS2z5L6RaIsoI1Q/edit?usp=sharing"

# Optional external URL for project (replaces project detail page).
external_link: https://github.com/zichenzhang04/slm-tot

image:
  caption: Proposed Pipeline
  focal_point: Smart
---
