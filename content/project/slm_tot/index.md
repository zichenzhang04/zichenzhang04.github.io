---
title: Babysitting a Small Language Model through One-Step Tree-of-Thoughts Knowledge Distillation

# subtitle: A Subtitle

summary: <div><p style=color:gray>Anurag Renduchintala*, Adi Mahesh*, <b>Zichen Zhang</b>*, Zimo Si*, Shangjun Meng*, Samuel Fang*.<br></p></div>We introduce the One-Step Tree-of-Thoughts (ToT) framework, a simplified prompting method that distills multi-step reasoning into a single structured prompt, and demonstrates how knowledge distillation can transfer this reasoning capability from Large Language Models to Small Language Models with much less parameters, enabling significant improvements in computational efficiency and reasoning performance, as shown on the model's performance on Game of 24.

tags:
  - NLP
  - LLM
  - ML

date: "2024-11-28"

links:

# Optional external URL for project (replaces project detail page).
external_link:

image:
  caption: Proposed Pipeline
  focal_point: Smart
---
